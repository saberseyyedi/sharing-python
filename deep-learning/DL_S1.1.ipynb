{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c2ba4f5-24e6-4450-8f62-168464549ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World\n"
     ]
    }
   ],
   "source": [
    "test = \"Hello World\"\n",
    "print(\"test:\", test)\n",
    "# Expected output: test: Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "298fe723-cf47-4d4b-8414-6aa519801f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5692c-8c9b-4435-b253-e888ae8ff6e0",
   "metadata": {},
   "source": [
    "# sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c588f49-7c9e-4896-a952-dc55bbc268d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.linspace(-10,10,100)\n",
    "b = 1/(1+np.exp(-a))\n",
    "\n",
    "plt.plot(a, b)\n",
    "plt.xlabel(\"A\")\n",
    "plt.ylabel(\"Sigmoid(A)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d7cf30d-b393-4a40-b352-24ad819774e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+math.exp(-x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7a64960-82d5-4336-b437-382c3ee7e4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852259683067269"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(4.2)\n",
    "# Expected output: 0.9852259683067269"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d9c94-cbdf-4e64-9505-b457beb760e2",
   "metadata": {},
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning is that we work on Matrix and Vector but math works on real numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d3ab1f0-67ff-4217-b2c6-728f84b626b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/2487809397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbasic_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15780/750808529.py\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbasic_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "basic_sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3a666b7-edd1-4501-add3-746d9088b444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7549bc3-8e43-45df-8305-3762ace6637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8586ce-2a54-448d-9c4c-93ea6562f6f8",
   "metadata": {},
   "source": [
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "786b736a-2c89-4c53-a827-90aa2b2536a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed19f22a-5c93-4c39-b42b-66deef3a72b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)\n",
    "# Expected output: array([0.73105858, 0.88079708, 0.95257413])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055994cd-686d-48aa-8973-ca3b5802960b",
   "metadata": {},
   "source": [
    "### Sigmoid gradient\n",
    "\n",
    "You will need to compute gradients to optimize loss functions using backpropagation. \n",
    "\n",
    "**Exercise**: The function sigmoid_grad() compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You often code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0568056-8923-4863-977e-d5c08d564cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Arguments: x  --> A scalar or numpy array\n",
    "    Return:    ds --> Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "    ds = s*(1-s)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80005685-cde4-4b24-be35-bd899d362f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (sigmoid_derivative(x))\n",
    "# Expected output: [0.19661193 0.10499359 0.04517666]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f41fe7-c35c-4344-bc71-ed70a15dd29c",
   "metadata": {},
   "source": [
    "### Reshaping arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d77eb31-0db2-426f-93a5-f88759c51d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument: image -- a numpy array of shape (length, height, depth)\n",
    "    Returns:  v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    v = image.reshape(3*3*2,1)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f2acb4f-408c-4268-9762-6cd6ecc15a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                   [ 0.90714982,  0.52835647],\n",
    "                   [ 0.4215251 ,  0.45017551]],\n",
    "                  [[ 0.92814219,  0.96677647],\n",
    "                   [ 0.85304703,  0.52351845],\n",
    "                   [ 0.19981397,  0.27417313]],\n",
    "                  [[ 0.60659855,  0.00533165],\n",
    "                   [ 0.10820313,  0.49978937],\n",
    "                   [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print(image2vector(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc84c11-0808-46d1-8d29-41934d4ff664",
   "metadata": {},
   "source": [
    "### Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daa988ba-3261-417a-b3e7-dc28da88b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Argument:     x -- A numpy matrix of shape (n, m)\n",
    "    Returns:      x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x,axis=1,keepdims=True)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    x = x/x_norm\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06467437-957b-4efb-8d63-daefc636d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(normalizeRows(x))\n",
    "# Expected output: [[0.         0.6        0.8       ]\n",
    "#                   [0.13736056 0.82416338 0.54944226]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7d155-10fe-418b-a82a-27a82cc98988",
   "metadata": {},
   "source": [
    "### Broadcasting and the softmax function\n",
    "**Exercise**: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes.\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc030357-98d8-4a42-9a12-8c684e987dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    The code works for a row vector and also for matrices of shape (n, m).\n",
    "    \"\"\"\n",
    "    \n",
    "    max = np.max(x,axis=1,keepdims=True)    #returns max of each row and keeps same dims\n",
    "    e_x = np.exp(x - max)                   #subtracts each row with its max value\n",
    "    sum = np.sum(e_x,axis=1,keepdims=True)  #returns sum of each row and keeps same dims\n",
    "    s = e_x / sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b46e7847-1609-4926-8ca9-d20dceef8405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[9, 2, 5, 0, 0],\n",
    "              [7, 5, 0, 0 ,0]])\n",
    "print(softmax(x))\n",
    "# Expected output: [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04 1.21052389e-04]\n",
    "#                   [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04 8.01252314e-04]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4127741a-30e9-4d46-81c2-287017a91581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9],\n",
       "       [7]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.max(x,axis=1,keepdims=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10669b49-4530-4eed-9bc1-8651fe4bf5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -7, -4, -9, -9],\n",
       "       [ 0, -2, -7, -7, -7]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x - a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd058246-8e72-4a87-8ef4-689c9c53088e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 9.11881966e-04, 1.83156389e-02, 1.23409804e-04,\n",
       "        1.23409804e-04],\n",
       "       [1.00000000e+00, 1.35335283e-01, 9.11881966e-04, 9.11881966e-04,\n",
       "        9.11881966e-04]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.exp(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5c73d3d-ee28-4d0f-b49a-9c4f378099aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 2 5 0 0]\n",
      " [7 5 0 0 0]]\n",
      "[[16]\n",
      " [12]]\n",
      "[[0.5625     0.125      0.3125     0.         0.        ]\n",
      " [0.58333333 0.41666667 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.sum(axis=1,keepdims=True))\n",
    "print(x/x.sum(axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89e7a6-2495-40cd-9f6a-b04202acc3f0",
   "metadata": {},
   "source": [
    "## 2 - Vectorization\n",
    "\n",
    "To make sure that your code is  computationally efficient, you will use vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "620af497-a62c-4ebc-b2c0-aa63c6b6f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dot product\" computation time = 0.0 ms\n",
      "\"outer product\" computation time = 890.625 ms\n",
      "\"elementwise multiplication\" computation time = 0.0 ms\n",
      "\"(general) dot product\" computation time = 4359.375 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1024)\n",
    "x2 = np.random.rand(1024)\n",
    "W = np.random.rand(4096, len(x1))\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('dot product', (toc - tic)*1e3))\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('outer product', (toc - tic)*1e3))\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('elementwise multiplication', (toc - tic)*1e3))\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('(general) dot product', (toc - tic)*1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b23e290-0c4a-4127-9cf5-a7b8f30c441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dot product\" computation time = 0.0 ms\n",
      "\"outer product\" computation time = 15.625 ms\n",
      "\"elementwise multiplication\" computation time = 0.0 ms\n",
      "\"(general) dot product\" computation time = 46.875 ms\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('dot product', (toc - tic)*1e3))\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('outer product', (toc - tic)*1e3))\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('elementwise multiplication', (toc - tic)*1e3))\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print('\"{}\" computation time = {} ms'.format('(general) dot product', (toc - tic)*1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827373b-72b4-41ae-90bf-d8a84e845ef0",
   "metadata": {},
   "source": [
    "### Implement the L1 and L2 loss functions\n",
    "\n",
    "The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01e8d444-9883-4921-ace0-c167541c9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \n",
    "    loss = np.sum(np.abs(yhat-y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e473480-0515-4150-9a77-e87935704740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print( L1(yhat, y) )\n",
    "# Expected output: 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfe36b-63d9-4129-9f57-41fb0ac2aa3d",
   "metadata": {},
   "source": [
    "Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43226588-e322-40ae-953b-f638aa00b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \n",
    "    x = yhat-y\n",
    "    loss = np.sum(np.dot(x,x))# TODO (â‰ˆ 1 LOC)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dced708b-ab32-4ed1-aca3-76703ba0f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print( L2(yhat,y) )\n",
    "# Expected output: 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bd980-6145-4599-b209-dfeb370fc93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
